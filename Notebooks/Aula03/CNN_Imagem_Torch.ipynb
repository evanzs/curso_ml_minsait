{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checando se a gpu está disponível senão roda na cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prática\n",
    "1. Aquisição e pré-processamento dos dados\n",
    "2. Treinamento\n",
    "* Implementar arquitetura\n",
    "* Definir otimizadores, métricas e regularizadores\n",
    "3. Teste (avaliação de desempenho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Aquisição dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "                                 transforms.ToTensor()\n",
    "])\n",
    " \n",
    "# Loading Data and splitting it into train and validation data\n",
    "train = datasets.MNIST('', train = True, transform = transforms, download = True)\n",
    "train, valid = random_split(train,[50000,10000])\n",
    " \n",
    "# Create Dataloader of the above tensor with batch size = 32\n",
    "trainloader = DataLoader(train, batch_size=32)\n",
    "validloader = DataLoader(valid, batch_size=32)\n",
    "\n",
    "testset = datasets.MNIST('', train=False,\n",
    "                                       download=True, transform=transforms)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1 , 3)\n",
    "        self.fc1 = nn.Linear(676, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "min_valid_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 0.218672568793632 \t\t Validation Loss: 0.2095297048517024\n",
      "Validation Loss Decreased(inf--->65.58279761858284) \t Saving The Model\n",
      "Epoch 2 \t\t Training Loss: 0.17647654295589724 \t\t Validation Loss: 0.1726793692396662\n",
      "Validation Loss Decreased(65.58279761858284--->54.048642572015524) \t Saving The Model\n",
      "Epoch 3 \t\t Training Loss: 0.15009658217372912 \t\t Validation Loss: 0.15906454823887387\n",
      "Validation Loss Decreased(54.048642572015524--->49.78720359876752) \t Saving The Model\n",
      "Epoch 4 \t\t Training Loss: 0.13246884585524446 \t\t Validation Loss: 0.14502416244342017\n",
      "Validation Loss Decreased(49.78720359876752--->45.39256284479052) \t Saving The Model\n",
      "Epoch 5 \t\t Training Loss: 0.11959675874802989 \t\t Validation Loss: 0.1380088684303811\n",
      "Validation Loss Decreased(45.39256284479052--->43.196775818709284) \t Saving The Model\n",
      "Epoch 6 \t\t Training Loss: 0.10929642036296212 \t\t Validation Loss: 0.13557842635615067\n",
      "Validation Loss Decreased(43.196775818709284--->42.436047449475154) \t Saving The Model\n",
      "Epoch 7 \t\t Training Loss: 0.10112530455783948 \t\t Validation Loss: 0.13255855280343598\n",
      "Validation Loss Decreased(42.436047449475154--->41.49082702747546) \t Saving The Model\n",
      "Epoch 8 \t\t Training Loss: 0.09381959478159912 \t\t Validation Loss: 0.13201886548038821\n",
      "Validation Loss Decreased(41.49082702747546--->41.32190489536151) \t Saving The Model\n",
      "Epoch 9 \t\t Training Loss: 0.08777605193790455 \t\t Validation Loss: 0.13189330007642888\n",
      "Validation Loss Decreased(41.32190489536151--->41.28260292392224) \t Saving The Model\n",
      "Epoch 10 \t\t Training Loss: 0.08218177665270489 \t\t Validation Loss: 0.13522196605878944\n",
      "Epoch 11 \t\t Training Loss: 0.07713999953201388 \t\t Validation Loss: 0.13691230049504105\n",
      "Epoch 12 \t\t Training Loss: 0.07200303599359056 \t\t Validation Loss: 0.1383477095417833\n",
      "Epoch 13 \t\t Training Loss: 0.06752206184410461 \t\t Validation Loss: 0.14161809580623747\n",
      "Epoch 14 \t\t Training Loss: 0.06302637977231984 \t\t Validation Loss: 0.14364787714612393\n",
      "Epoch 15 \t\t Training Loss: 0.0586688506559386 \t\t Validation Loss: 0.14968371175388737\n",
      "Epoch 16 \t\t Training Loss: 0.05554022758289569 \t\t Validation Loss: 0.15384989195077803\n",
      "Epoch 17 \t\t Training Loss: 0.05158489307392611 \t\t Validation Loss: 0.16137465061627315\n",
      "Epoch 18 \t\t Training Loss: 0.04773216839170385 \t\t Validation Loss: 0.16718636160402503\n",
      "Epoch 19 \t\t Training Loss: 0.044206310495139775 \t\t Validation Loss: 0.1722016726447918\n",
      "Epoch 20 \t\t Training Loss: 0.04111098463815434 \t\t Validation Loss: 0.18191174780946232\n",
      "Epoch 21 \t\t Training Loss: 0.03820783106835449 \t\t Validation Loss: 0.19141564096867913\n",
      "Epoch 22 \t\t Training Loss: 0.035217867274680764 \t\t Validation Loss: 0.1991664749969462\n",
      "Epoch 23 \t\t Training Loss: 0.0324247841488012 \t\t Validation Loss: 0.20241889973303465\n",
      "Epoch 24 \t\t Training Loss: 0.030529732423969242 \t\t Validation Loss: 0.21513525722632051\n",
      "Epoch 25 \t\t Training Loss: 0.028051063961685472 \t\t Validation Loss: 0.21645973833220544\n",
      "Epoch 26 \t\t Training Loss: 0.026065971088257846 \t\t Validation Loss: 0.2277902385524964\n",
      "Epoch 27 \t\t Training Loss: 0.024284772670193116 \t\t Validation Loss: 0.24403027890412993\n",
      "Epoch 28 \t\t Training Loss: 0.025150302013470825 \t\t Validation Loss: 0.24181553633684053\n",
      "Epoch 29 \t\t Training Loss: 0.02158819652886997 \t\t Validation Loss: 0.2591873514688625\n",
      "Epoch 30 \t\t Training Loss: 0.022626223613918944 \t\t Validation Loss: 0.2697912650175637\n",
      "Epoch 31 \t\t Training Loss: 0.019967048337714607 \t\t Validation Loss: 0.27451033083738824\n",
      "Epoch 32 \t\t Training Loss: 0.01877357191272635 \t\t Validation Loss: 0.2769313644614678\n",
      "Epoch 33 \t\t Training Loss: 0.019723128611359116 \t\t Validation Loss: 0.2816210942246965\n",
      "Epoch 34 \t\t Training Loss: 0.017912563227029346 \t\t Validation Loss: 0.2774637258771538\n",
      "Epoch 35 \t\t Training Loss: 0.01765033396691717 \t\t Validation Loss: 0.2936866640848776\n",
      "Epoch 36 \t\t Training Loss: 0.016382007655076865 \t\t Validation Loss: 0.2750555690455641\n",
      "Epoch 37 \t\t Training Loss: 0.016024082348863863 \t\t Validation Loss: 0.29456900011244075\n",
      "Epoch 38 \t\t Training Loss: 0.014505150990562536 \t\t Validation Loss: 0.29494845090073524\n",
      "Epoch 39 \t\t Training Loss: 0.014433225696370872 \t\t Validation Loss: 0.30293711427848014\n",
      "Epoch 40 \t\t Training Loss: 0.013886638465507627 \t\t Validation Loss: 0.3109818452040598\n",
      "Epoch 41 \t\t Training Loss: 0.013427648178632217 \t\t Validation Loss: 0.32407731214219493\n",
      "Epoch 42 \t\t Training Loss: 0.012791339174216427 \t\t Validation Loss: 0.32412557220114946\n",
      "Epoch 43 \t\t Training Loss: 0.013203509333103713 \t\t Validation Loss: 0.32060936243791816\n",
      "Epoch 44 \t\t Training Loss: 0.011564436578336482 \t\t Validation Loss: 0.30974617451045033\n",
      "Epoch 45 \t\t Training Loss: 0.012109558263134536 \t\t Validation Loss: 0.33316380455959527\n",
      "Epoch 46 \t\t Training Loss: 0.011862255620109792 \t\t Validation Loss: 0.33685782255180163\n",
      "Epoch 47 \t\t Training Loss: 0.010234220614099152 \t\t Validation Loss: 0.34338021249506345\n",
      "Epoch 48 \t\t Training Loss: 0.01117366668207803 \t\t Validation Loss: 0.3492080589348017\n",
      "Epoch 49 \t\t Training Loss: 0.01162040806262892 \t\t Validation Loss: 0.3434990576569712\n",
      "Epoch 50 \t\t Training Loss: 0.011776976808885072 \t\t Validation Loss: 0.3370825460114114\n",
      "Epoch 51 \t\t Training Loss: 0.011363554894771668 \t\t Validation Loss: 0.3587580373581707\n",
      "Epoch 52 \t\t Training Loss: 0.008182437753044438 \t\t Validation Loss: 0.3767475586116721\n",
      "Epoch 53 \t\t Training Loss: 0.010263546220266687 \t\t Validation Loss: 0.35083235884653735\n",
      "Epoch 54 \t\t Training Loss: 0.010211333519411446 \t\t Validation Loss: 0.3783896601433875\n",
      "Epoch 55 \t\t Training Loss: 0.0073893992823924 \t\t Validation Loss: 0.37401065653927473\n",
      "Epoch 56 \t\t Training Loss: 0.009596134836751136 \t\t Validation Loss: 0.3556248205337658\n",
      "Epoch 57 \t\t Training Loss: 0.008517494406915212 \t\t Validation Loss: 0.37697878412297386\n",
      "Epoch 58 \t\t Training Loss: 0.008121249334165498 \t\t Validation Loss: 0.3835174354035089\n",
      "Epoch 59 \t\t Training Loss: 0.007519969079504551 \t\t Validation Loss: 0.3718365852438949\n",
      "Epoch 60 \t\t Training Loss: 0.008684064554019733 \t\t Validation Loss: 0.3690230045933618\n",
      "Epoch 61 \t\t Training Loss: 0.008659671189801165 \t\t Validation Loss: 0.35344853363315676\n",
      "Epoch 62 \t\t Training Loss: 0.007492874170309309 \t\t Validation Loss: 0.37984629046164\n",
      "Epoch 63 \t\t Training Loss: 0.007242489944946166 \t\t Validation Loss: 0.4044266564649709\n",
      "Epoch 64 \t\t Training Loss: 0.008550260838583679 \t\t Validation Loss: 0.39518379511150614\n",
      "Epoch 65 \t\t Training Loss: 0.00757878235995817 \t\t Validation Loss: 0.39534671422137413\n",
      "Epoch 66 \t\t Training Loss: 0.007134600338446148 \t\t Validation Loss: 0.41141495180508514\n",
      "Epoch 67 \t\t Training Loss: 0.008970268589798349 \t\t Validation Loss: 0.3857277452667249\n",
      "Epoch 68 \t\t Training Loss: 0.0067186971855860275 \t\t Validation Loss: 0.38655202046686515\n",
      "Epoch 69 \t\t Training Loss: 0.007154427083771485 \t\t Validation Loss: 0.40205063939902086\n",
      "Epoch 70 \t\t Training Loss: 0.006637225167121384 \t\t Validation Loss: 0.38647290463452777\n",
      "Epoch 71 \t\t Training Loss: 0.00730286319370464 \t\t Validation Loss: 0.3867095391330544\n",
      "Epoch 72 \t\t Training Loss: 0.007175065646071124 \t\t Validation Loss: 0.38759768217489565\n",
      "Epoch 73 \t\t Training Loss: 0.006422352394616127 \t\t Validation Loss: 0.41159768705399064\n",
      "Epoch 74 \t\t Training Loss: 0.007792604920056884 \t\t Validation Loss: 0.40761481167561203\n",
      "Epoch 75 \t\t Training Loss: 0.005131327562055436 \t\t Validation Loss: 0.41072089328126726\n",
      "Epoch 76 \t\t Training Loss: 0.007742484395383293 \t\t Validation Loss: 0.40228005124456445\n",
      "Epoch 77 \t\t Training Loss: 0.005263436526061845 \t\t Validation Loss: 0.43284089140962506\n",
      "Epoch 78 \t\t Training Loss: 0.006717195856662392 \t\t Validation Loss: 0.39106540829295144\n",
      "Epoch 79 \t\t Training Loss: 0.006158520182924535 \t\t Validation Loss: 0.38487285421207135\n",
      "Epoch 80 \t\t Training Loss: 0.0052586723164806835 \t\t Validation Loss: 0.42304314178649843\n",
      "Epoch 81 \t\t Training Loss: 0.006408413483484973 \t\t Validation Loss: 0.40709252431913395\n",
      "Epoch 82 \t\t Training Loss: 0.0038503581883316054 \t\t Validation Loss: 0.4030050546789541\n",
      "Epoch 83 \t\t Training Loss: 0.0074041387556288245 \t\t Validation Loss: 0.42638822997667114\n",
      "Epoch 84 \t\t Training Loss: 0.007725430488919844 \t\t Validation Loss: 0.40330786559674725\n",
      "Epoch 85 \t\t Training Loss: 0.003988760744750606 \t\t Validation Loss: 0.4096989641812065\n",
      "Epoch 86 \t\t Training Loss: 0.005494555811790575 \t\t Validation Loss: 0.40025380192902194\n",
      "Epoch 87 \t\t Training Loss: 0.005667409840192888 \t\t Validation Loss: 0.4029757953580628\n",
      "Epoch 88 \t\t Training Loss: 0.004697549331669467 \t\t Validation Loss: 0.4141879434936194\n",
      "Epoch 89 \t\t Training Loss: 0.006755238524086935 \t\t Validation Loss: 0.41153638751345595\n",
      "Epoch 90 \t\t Training Loss: 0.0047525848189969916 \t\t Validation Loss: 0.4216264559804789\n",
      "Epoch 91 \t\t Training Loss: 0.006229460102945849 \t\t Validation Loss: 0.43525118999420703\n",
      "Epoch 92 \t\t Training Loss: 0.004831932641331417 \t\t Validation Loss: 0.4445859781465536\n",
      "Epoch 93 \t\t Training Loss: 0.004630019770105941 \t\t Validation Loss: 0.4391150505000508\n",
      "Epoch 94 \t\t Training Loss: 0.0048036427082126035 \t\t Validation Loss: 0.41581701525428494\n",
      "Epoch 95 \t\t Training Loss: 0.004515603630595862 \t\t Validation Loss: 0.420265815535962\n",
      "Epoch 96 \t\t Training Loss: 0.006026939759571133 \t\t Validation Loss: 0.4334758051406534\n",
      "Epoch 97 \t\t Training Loss: 0.0035052575410905733 \t\t Validation Loss: 0.4228479132898364\n",
      "Epoch 98 \t\t Training Loss: 0.005634082110390829 \t\t Validation Loss: 0.43967475885363483\n",
      "Epoch 99 \t\t Training Loss: 0.002947776557372059 \t\t Validation Loss: 0.4627767549642278\n",
      "Epoch 100 \t\t Training Loss: 0.004842974206419813 \t\t Validation Loss: 0.45724043278108556\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    train_loss = 0.0\n",
    "    for data, labels in trainloader:\n",
    "        # Transfer Data to GPU if available\n",
    "        # if torch.cuda.is_available():\n",
    "        #     data, labels = data.cuda(), labels.cuda()\n",
    "         \n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward Pass\n",
    "        target = net(data)\n",
    "        # Find the Loss\n",
    "        loss = criterion(target,labels)\n",
    "        # Calculate gradients \n",
    "        loss.backward()\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        # Calculate Loss\n",
    "        train_loss += loss.item()\n",
    "     \n",
    "    valid_loss = 0.0\n",
    "    net.eval()     # Optional when not using Model Specific layer\n",
    "    for data, labels in validloader:\n",
    "        # Transfer Data to GPU if available\n",
    "        # if torch.cuda.is_available():\n",
    "        #     data, labels = data.cuda(), labels.cuda()\n",
    "         \n",
    "        # Forward Pass\n",
    "        target = net(data)\n",
    "        # Find the Loss\n",
    "        loss = criterion(target,labels)\n",
    "        # Calculate Loss\n",
    "        valid_loss += loss.item()\n",
    "    training_loss_print = train_loss / len(trainloader)\n",
    "    valid_loss_print = valid_loss / len(validloader)\n",
    "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {training_loss_print} \\t\\t Validation Loss: {valid_loss_print}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(net.state_dict(), 'saved_model.pth')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96875"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "accuracy_score(labels, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valor_venal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
